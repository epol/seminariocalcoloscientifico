\documentclass[a4paper,10pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{xfrac}
\usepackage[all]{xy}
\usepackage{graphicx}
%\usepackage{fullpage}
\usepackage{hyperref}
\usepackage[utf8x]{inputenc}
\usepackage[italian]{babel}

%\setlength{\parindent}{0in}

\newcounter{counter1}

\theoremstyle{plain}
\newtheorem{myteo}[counter1]{Teorema}
\newtheorem{mylem}[counter1]{Lemma}
\newtheorem{mypro}[counter1]{Proposizione}
\newtheorem{mycor}[counter1]{Corollario}
\newtheorem*{myteo*}{Teorema}
\newtheorem*{mylem*}{Lemma}
\newtheorem*{mypro*}{Proposizione}
\newtheorem*{mycor*}{Corollario}

\theoremstyle{definition}
\newtheorem{mydef}[counter1]{Definizione}
\newtheorem{myes}[counter1]{Esempio}
\newtheorem{myex}[counter1]{Esercizio}
\newtheorem*{mydef*}{Definizione}
\newtheorem*{myes*}{Esempio}
\newtheorem*{myex*}{Esercizio}

\theoremstyle{remark}
\newtheorem{mynot}[counter1]{Nota}
\newtheorem{myoss}[counter1]{Osservazione}
\newtheorem*{mynot*}{Nota}
\newtheorem*{myoss*}{Osservazione}


\newcommand{\obar}[1]{\overline{#1}}
\newcommand{\ubar}[1]{\underline{#1}}

\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\pa}[1]{\left(#1\right)}
\newcommand{\ang}[1]{\left<#1\right>}
\newcommand{\bra}[1]{\left[#1\right]}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\left\|#1\right\|}

\newcommand{\pfrac}[2]{\pa{\frac{#1}{#2}}}
\newcommand{\bfrac}[2]{\bra{\frac{#1}{#2}}}
\newcommand{\psfrac}[2]{\pa{\sfrac{#1}{#2}}}
\newcommand{\bsfrac}[2]{\bra{\sfrac{#1}{#2}}}

\newcommand{\der}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pder}[2]{\pfrac{\partial #1}{\partial #2}}
\newcommand{\sder}[2]{\sfrac{\partial #1}{\partial #2}}
\newcommand{\psder}[2]{\psfrac{\partial #1}{\partial #2}}

\newcommand{\intl}{\int \limits}

\DeclareMathOperator{\de}{d}
\DeclareMathOperator{\id}{Id}
\DeclareMathOperator{\len}{len}

\DeclareMathOperator{\gl}{GL}
\DeclareMathOperator{\aff}{Aff}
\DeclareMathOperator{\isom}{Isom}

\DeclareMathOperator{\im}{Im}




\title{Seminario Calcolo Scientifico - appunti1}
\author{Enrico Polesel}
\date{\today}

\begin{document}
\maketitle

\section{Contest}
\label{sec:contest}

\subsection{Fredholm integral equations}
\label{sec:fredholm}

Many physical problems can be modeled with the equation
\begin{equation}
  \label{eq:fredholm}
  g(t) = \intl _{\Omega} k(t,s) f(s) \de s
\end{equation}
where $f(s)\in L^2(\Omega)$ is the function that we want to know and
$g(t)\in L^2(\Omega)$ is the measure we can get, $k(t,s) \in
L^2(\Omega \times \Omega)$ is called \textit{kernel} and it is given
by the model.

So we have the linear funcion
\[ \begin{matrix}
  K:\; &L^2(\Omega) &\longrightarrow &L^2(\Omega)\\
  & f &\longrightarrow & \intl _\Omega k(\cdot , s) f(s) \de s
  \end{matrix}
\]

For the Riemann-Lebesgue lemma we have that $K$ smooths the higher
frequency components of $f$, in fact if we have $\Omega = \mathbb{R}$
then
\[ \lim _{z \to \infty} \intl _\mathbb{R} k(t,s) e^{-zs} \de s = 0 \]

Under certain hypothesis ($K$ compact) we can write the Singualr Value
Expansion (SVE) of $K$, so there exist $\set{v_i}_{i\in \mathbb{N}}$
orthonormal basis of $L^2(\Omega)$, $\set{v_i}_{i\in \mathbb{N}}$
orthonormal basis of $\im(K)$, $\set{\sigma _i}_{i\in \mathbb{N}}$
nonnegative set of singualar values such that
\begin{equation}
  \label{eq:SVErealtion}
  K(v_j) = \sigma _j u_j
\end{equation}
We also have $\sigma _i > \sigma _{i+1}$ and $i \to \infty \Rightarrow
\sigma _i \to 0$.

So we can write
\begin{equation}
  \label{eq:SVE}
  K(f) = \sum _{i\in \mathbb{N}} u_i \sigma _i \ang{f,v_i}
\end{equation}

It is known that with the increasing of $i$ the functions $v_i$ and
$u_i$ becomes more and more oscillating, the fact that $\sigma _i \to
0$ dumps that high frequency components when applicating $K$.


If we assume that $K$ is injective we can define $K^{-1}:\; \im(K) \to
L^2(\Omega)$, our problem now is to calculate $K^{-1}(g(t))$ where
$g(t)$ is our measure.

Recalling the SVE of $K$ we can write (for $g\in \im(K)$)
\begin{equation}
  \label{eq:inverseSVE}
  K^{-1}(g) = \sum _{i\in \mathbb{N}} v_i \frac{1}{\sigma _i} \ang{g,u_i}
\end{equation}
so $K^{-1}$ has a amplifying effect on the higher frequency components
of $g$.

We have (we won't prove this fact) that $g\in \im (K)$ if and only if
it satisfies the Picard condition:
\[ \sum _{i\in \mathbb{N}} \pa{ \frac{1}{\sigma _i} \ang{g,u_i}} ^2 <
\infty \] 

So if we measure $\tilde g + \varepsilon$ where $\varepsilon$ is a
small perturbation given by sperimental errors we cannot expect
$\tilde g$ to satisfy Picar condition, so simply inverting $K$ without
any regularization would be useless.


\subsubsection{Discretization}
\label{sec:fredholmdiscretization}

To to solve numerically the Fredholm problem we have do discretize it,
we can choose to regularize the problem before or after the
regularization.

Common discretization methods are collocation methods and projection
methods.

If we discretize the problem before regularizing it we'll get a bad
conditioned matrix, so we have reduced the original regularization
problem to the regularization of a bad conditioned matrix.

\subsection{Linear (finite) system}
\label{sec:exmatrix}

Let $A\in \mathbb{R}^{m \times n}$ be a matrix with $k \ge n$ and $b
\in \mathbb{R}^m$, we may want to find a vector $x \in \mathbb{R}^n$
such that $Ax = b$, this problem is well-defined only if $b\in \im
(A)$, so a more general request is to find
\begin{equation}
  \label{eq:minxunreg}
  x = \arg\min _{x\in \mathbb{R}^n} \pa{ \norm{Ax -b} }
\end{equation}
for some norm $\norm{\cdot}$, we will use $\norm{\cdot}_2$.

We can obtain this equation from various problems, we have see the
case of the discretization of a Fredholm equation, another common case
is a step in the Newton method to find the zero of a nonlinear
function, if the Jacobian matrix is bad conditioned then we will have
to regularize to obtain a ``good'' decresing direction.

Examples of this can be found in \cite{inversesturm} and \cite{atmospheric}

We can write the SVD of $A$, so there exist $U\in \mathbb{R}^{m\times
  m}$, $V\in \mathbb{R}^{n\times n}$ orthogonal and $\set{\sigma
  _i}_{i=1..n}$ nonnegative with $\sigma _i > \sigma _{i+1}$ such that
\begin{equation}
  \label{eq:SVDsum}
  Ax = \sum _{i=1} ^n u_i \sigma _i \ang{x,v_i}
\end{equation}
If we define
\[ \Sigma = \begin{pmatrix}
\sigma _1 & & & \\
& \sigma _2 & &  \\
& & \ddots & \\
& & & \sigma _n 
\end{pmatrix} \]
we can rewrite the \ref{eq:SVDsum} condition as
\begin{equation}
  \label{eq:SVDmat}
  A = U\Sigma V^T
\end{equation}
that is the Singular Values Decomposition of $A$.

We can now write the Moore-Penrose pseudoinverse of $A$ as
\begin{equation}
  \label{eq:pseudoinverse}
  A^+ = \sum _{i=1} ^k \frac{1}{\sigma _i} v_i u_i ^T 
\end{equation}
where $k$ is the rank of $A$ that can be written as
\[ k = \max \set{ i :\; \sigma_i \neq 0 } \]

Using the pseudoinverse it is possible to prove that
\begin{equation}
  \label{eq:inverseSVDunreg}
  x = \sum _{i=1} ^k \frac{1}{\sigma _i} v_i \ang{u_i,b}
\end{equation}
is the solution of our minimization problem.

%TODO: rewrite
From the definition of condition number we know that if the matrix $A$
is bad conditioned then a noisy vector $\tilde b = b + \varepsilon$
with $\varepsilon$ small can cause a big error in $\tilde x = \arg\min _{x}
\pa{ \norm{ Ax - \tilde b }}$

noisy vector $\tilde b = b + \varepsilon$
can change a lot the solution $\tilde x$.


A well know propriety of the SVD is that the condition number of $A$
can be written as
\[ \kappa (A) = \norm{A^+}_2\norm{A}_2 = \frac{\sigma _1}{\sigma _k} \]

So if the matrix $A$ is bad conditioned then some of the singular
values (at least $\sigma _k$) will be ``small'' and so regularization
will have to mitigate this problem.

\newpage
\bibliographystyle{alpha}
\bibliography{inversesturm,atmospheric}


\end{document}

