\documentclass[a4paper,10pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{xfrac}
\usepackage[all]{xy}
\usepackage{graphicx}
%\usepackage{fullpage}
\usepackage{hyperref}
\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}

%\setlength{\parindent}{0in}

\newcounter{counter1}

\theoremstyle{plain}
\newtheorem{myteo}[counter1]{Teorema}
\newtheorem{mylem}[counter1]{Lemma}
\newtheorem{mypro}[counter1]{Proposizione}
\newtheorem{mycor}[counter1]{Corollario}
\newtheorem*{myteo*}{Teorema}
\newtheorem*{mylem*}{Lemma}
\newtheorem*{mypro*}{Proposizione}
\newtheorem*{mycor*}{Corollario}

\theoremstyle{definition}
\newtheorem{mydef}[counter1]{Definition}
\newtheorem{myes}[counter1]{Esempio}
\newtheorem{myex}[counter1]{Esercizio}
\newtheorem*{mydef*}{Definition}
\newtheorem*{myes*}{Esempio}
\newtheorem*{myex*}{Esercizio}

\theoremstyle{remark}
\newtheorem{mynot}[counter1]{Nota}
\newtheorem{myoss}[counter1]{Osservazione}
\newtheorem*{mynot*}{Nota}
\newtheorem*{myoss*}{Osservazione}


\newcommand{\obar}[1]{\overline{#1}}
\newcommand{\ubar}[1]{\underline{#1}}

\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\pa}[1]{\left(#1\right)}
\newcommand{\ang}[1]{\left<#1\right>}
\newcommand{\bra}[1]{\left[#1\right]}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\left\|#1\right\|}

\newcommand{\pfrac}[2]{\pa{\frac{#1}{#2}}}
\newcommand{\bfrac}[2]{\bra{\frac{#1}{#2}}}
\newcommand{\psfrac}[2]{\pa{\sfrac{#1}{#2}}}
\newcommand{\bsfrac}[2]{\bra{\sfrac{#1}{#2}}}

\newcommand{\der}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pder}[2]{\pfrac{\partial #1}{\partial #2}}
\newcommand{\sder}[2]{\sfrac{\partial #1}{\partial #2}}
\newcommand{\psder}[2]{\psfrac{\partial #1}{\partial #2}}

\newcommand{\intl}{\int \limits}

\DeclareMathOperator{\de}{d}
\DeclareMathOperator{\id}{Id}
\DeclareMathOperator{\len}{len}

\DeclareMathOperator{\gl}{GL}
\DeclareMathOperator{\aff}{Aff}
\DeclareMathOperator{\isom}{Isom}

\DeclareMathOperator{\im}{Im}




\title{Seminario Calcolo Scientifico - appunti1}
\author{Enrico Polesel}
\date{\today}

\begin{document}
\maketitle

\section{Context}
\label{sec:context}

\subsection{Fredholm integral equations}
\label{sec:fredholm}

Many physical problems can be modeled with the equation
\begin{equation}
  \label{eq:fredholm}
  g(t) = \intl _{\Omega} k(t,s) f(s) \de s
\end{equation}
where $f(s)\in L^2(\Omega)$ is the function that we want to know and
$g(t)\in L^2(\Omega)$ is the measure we can get, $k(t,s) \in
L^2(\Omega \times \Omega)$ is called \textit{kernel} and it is given
by the model.

So we have the linear funcion
\[ \begin{matrix}
  K:\; &L^2(\Omega) &\longrightarrow &L^2(\Omega)\\
  & f &\longrightarrow & \intl _\Omega k(\cdot , s) f(s) \de s
  \end{matrix}
\]

For the Riemann-Lebesgue lemma we have that $K$ smooths the higher
frequency components of $f$, in fact if we have $\Omega = \mathbb{R}$
then
\[ \lim _{z \to \infty} \intl _\mathbb{R} k(t,s) e^{-zs} \de s = 0 \]

Under certain hypothesis ($K$ compact) we can write the Singualr Value
Expansion (SVE) of $K$, so there exist $\set{v_i}_{i\in \mathbb{N}}$
orthonormal basis of $L^2(\Omega)$, $\set{u_i}_{i\in \mathbb{N}}$
orthonormal basis of $\im(K)$, $\set{\sigma _i}_{i\in \mathbb{N}}$
nonnegative set of singualar values such that
\begin{equation}
  \label{eq:SVErealtion}
  K(v_j) = \sigma _j u_j
\end{equation}
We also have $\sigma _i \ge \sigma _{i+1}$ and $i \to \infty \Rightarrow
\sigma _i \to 0$.

So we can write
\begin{equation}
  \label{eq:SVE}
  K(f) = \sum _{i\in \mathbb{N}} u_i \sigma _i \ang{f,v_i}
\end{equation}

It is known that with the increasing of $i$ the functions $v_i$ and
$u_i$ becomes more and more oscillating, in fact that $\sigma _i \to
0$ dumps that high frequency components when applicating $K$.


If we assume that $K$ is injective then we can define $K^{-1}:\;
\im(K) \to L^2(\Omega)$, our problem now is to calculate
$K^{-1}(g(t))$ where $g(t)$ is our measure.

Recalling the SVE of $K$ we can write (for $g\in \im(K)$)
\begin{equation}
  \label{eq:inverseSVE}
  K^{-1}(g) = \sum _{i\in \mathbb{N}} v_i \frac{1}{\sigma _i} \ang{g,u_i}
\end{equation}
so $K^{-1}$ has a amplifying effect on the higher frequency components
of $g$.

We have (we won't prove this fact) that $g\in \im (K)$ if and only if
it satisfies the Picard condition:
\[ \sum _{i\in \mathbb{N}} \pa{ \frac{1}{\sigma _i} \ang{g,u_i}} ^2 <
\infty \] 

So if we measure $\tilde g + \varepsilon$ where $\varepsilon$ is a
small perturbation given by sperimental errors we cannot expect
$\tilde g$ to satisfy Picar condition, so simply inverting $K$ without
any regularization would be useless.


\subsubsection{Discretization}
\label{sec:fredholmdiscretization}

To to solve numerically the Fredholm problem we have do discretize it,
we can choose to regularize the problem before or after the
regularization.

Common discretization methods are collocation methods and projection
methods.

If we discretize the problem before regularizing it we'll get a bad
conditioned matrix, so we have reduced the original regularization
problem to the regularization of a bad conditioned matrix.

\subsection{Linear (finite) system}
\label{sec:exmatrix}

Let $A\in \mathbb{R}^{m \times n}$ be a matrix with $m \ge n$ and $b
\in \mathbb{R}^m$, we may want to find a vector $x \in \mathbb{R}^n$
such that $Ax = b$, this problem is defined (but not necessarily
well-posed) if and only if $b\in \im (A)$, so a more general request
is to find
\begin{equation}
  \label{eq:minxunreg}
  x = \arg\min _{x\in \mathbb{R}^n} \pa{ \norm{Ax -b} }
\end{equation}
for some norm $\norm{\cdot}$. We will use $\norm{\cdot}_2$.

We can obtain this equation from various problems, we have see the
case of the discretization of a Fredholm equation, another common case
is the step in the Newton method to find the zero of a nonlinear
function where the decreasing direction is calculated through the
inverse of the Jacobian matrix calculated at the point.

Examples of this can be found in \cite{inversesturm} and \cite{atmospheric}

We can write the SVD of $A$, so there exist $U\in \mathbb{R}^{m\times
  m}$, $V\in \mathbb{R}^{n\times n}$ orthogonal and $\set{\sigma
  _i}_{i=1..n}$ nonnegative with $\sigma _i \ge \sigma _{i+1}$ such that
\begin{equation}
  \label{eq:SVDsum}
  Ax = \sum _{i=1} ^n u_i \sigma _i \ang{v_i,x}
\end{equation}
If we define
\[ \Sigma = \begin{pmatrix}
\sigma _1 & & & \\
& \sigma _2 & &  \\
& & \ddots & \\
& & & \sigma _n 
\end{pmatrix} \]
we can rewrite the \ref{eq:SVDsum} condition as
\begin{equation}
  \label{eq:SVDmat}
  A = U\Sigma V^T
\end{equation}
that is the Singular Values Decomposition of $A$.

We can now write the Moore-Penrose pseudoinverse of $A$ as
\begin{equation}
  \label{eq:pseudoinverse}
  A^+ = \sum _{i=1} ^k \frac{1}{\sigma _i} v_i u_i ^T 
\end{equation}
where $k$ is the rank of $A$ that can be written as
\[ k = \max \set{ i :\; \sigma_i \neq 0 } \]

Using the pseudoinverse it is possible to prove that
\begin{equation}
  \label{eq:inverseSVDunreg}
  x = \sum _{i=1} ^k \frac{1}{\sigma _i} v_i \ang{u_i,b}
\end{equation}
is the solution of our minimization problem.

%TODO: rewrite
From the definition of condition number we know that if the matrix $A$
is bad conditioned then a noisy vector $\tilde b = b + \varepsilon$
with $\varepsilon$ small can cause a big error in $\tilde x = \arg\min _{x}
\pa{ \norm{ Ax - \tilde b }}$.

A well know propriety of the SVD is that the condition number of $A$
can be written as
\[ \kappa (A) = \norm{A^+}_2\norm{A}_2 = \frac{\sigma _1}{\sigma _k} \]

So if the matrix $A$ is bad conditioned then some of the singular
values (at least $\sigma _k$) will be ``small'' and so regularization
will have to mitigate this problem.

\section{Regularization methods}
\label{sec:regularizationmethods}

Let $A\in \mathbb{R}^{m \times n}$ be a bad conditioned matrix, we
want to find a ``good'' solution for \ref{eq:minxunreg}, i.e. that
doesn't depend too much on the noise of the input vector $\tilde b$.

Although it is impossible if we ask $\tilde x$ to be a solution for
\ref{eq:minxunreg}, we can modify slightly the problem to obtain a
stable result.

A common technique to analyze a regularization method is to write its
filter factors.

\begin{mydef}
  We say that a regularization method applied to a matrix $A = U
  \Sigma V^T$ has filter factors $\pa{\varphi _i}$ if and only if the
  regularized solution can be written has
  \begin{equation}
    \label{eq:filterdef}
    x = \sum _{i=1} ^k \varphi _i \frac{1}{\sigma _i} v_i
    \ang{u_i,\tilde b}
  \end{equation}
\end{mydef}

\subsection{TSVD}
\label{sec:TSVD}

Since we observed that the condition number of $A$ is
\[ \kappa (A) = \frac{\sigma _1}{\sigma _k} \] we may be able to
reduce it simpy ignoring some tiny $\sigma _i$, so if we choose a
integer parameter $\alpha \le k$ we can consider the matrix

\begin{equation}
  \label{eq:TSVDmatrix}
  A_\alpha = \sum _{i=1} ^\alpha u_i \sigma _i v_i ^T
\end{equation}

The conditioning number will be
\[ \kappa (A_\alpha) = \frac{\sigma _1}{\sigma _\alpha} \le  \frac{\sigma
  _1}{\sigma _k} = \kappa(A) \]
since $\sigma _\alpha \ge \sigma _k$.

So the solution is
\begin{equation}
  \label{eq:TSVDsolution}
  x_\alpha = A_\alpha ^+ \tilde b = \sum _{i=1} ^\alpha \frac{1}{\sigma _i}
  v_i \ang{u_i,\tilde b}
\end{equation}

And the filter factors are:
\begin{equation}
  \label{eq:TSVDfilter}
  \varphi ^{\text{TSVD}}_{\alpha,i} = \left\{
    \begin{matrix}
      1 \; &1\le i \le \alpha\\
      0 \; &\alpha < i \le k
    \end{matrix}
    \right.
\end{equation}

If we choose $\alpha$ too large then $A_\alpha$ will be too bad
conditioned, if we choose $\alpha$ too small then $A_\alpha$ will be
too different from $A$ and it won't be a good representation of the
original problem. We will see in section \ref{sec:discrepancy} a
method to choose a good regularization parameter $\alpha$.

The disadvantage of this approach is that it completely ignores the
components $u_i, i>\alpha$ in $\tilde b$.

\subsection{Tikhonov regularization}
\label{sec:tikhonov}

Since a too small singular value $\sigma _i$ amplifies ``too much''
the component $u_i$ of the measure $\tilde b$, a regularization method
may be to modify slightly the problem \ref{eq:minxunreg} in
\begin{equation}
  \label{eq:minxtiki}
  x_\mu = \arg\min _{x \in \mathbb{R}^n} \pa{ \norm{Ax -\tilde b}^2 + \mu\norm{x}^2}
\end{equation}

Tuning the parameter $\mu$ permits to penalize non regular
solutions. Also in this case we have the problem to choice a ``good''
regularization parameter $\mu$, in fact a too small $\mu$ gives an
unregularized solution, a too big $\mu$ would change too much the
problem and it wouldn't fit anymore the model.

A more general form of the Tikhonov regularization is
\begin{equation}
  \label{eq:minxtikgen}
  x_\mu = \arg\min _{x \in \mathbb{R}^n} \pa{ \norm{Ax -\tilde b}^2 +
    \norm{L_\mu x}^2}
\end{equation}
where $L_\mu$ is some penalizing matrix. If we choose $L_\mu = \mu
I$ then we will get the original Tikhonov form \ref{eq:minxtiki}.

The latter form is usually used when a good penalizing matrix $L$ is
known (for example if we know in wich way the solution has to be
``smooth'') setting $L_\mu = \mu L$. We will assume to not know that
matrix $L$, so we would use $L_\mu = \mu I$.

We can rewrite the realtion \ref{eq:minxtikgen} as
\begin{equation*}
  x_\mu = \arg\min _{x \in \mathbb{R}^n} \pa{ \norm{ 
      \begin{pmatrix}
        A \\
        L_\mu
      \end{pmatrix}
      x_\mu -
      \begin{pmatrix}
        \tilde b\\
        0
      \end{pmatrix}
      }^2 }
\end{equation*}
and we know that the solution is given by the equation
\begin{equation*}
  \begin{pmatrix}
    A^T & L_\mu ^T
  \end{pmatrix}
  \begin{pmatrix}
    A \\
    L_\mu
  \end{pmatrix}
  x_\mu = 
  \begin{pmatrix}
    A^T & L_\mu ^T
  \end{pmatrix}
  \begin{pmatrix}
    \tilde b\\
    0
  \end{pmatrix}
\end{equation*}
that can be rewritten as
\begin{equation*}
  \pa{ A^T A + L_\mu^T L_\mu } x_\mu = A^T \tilde b 
\end{equation*}
So the solution is
\begin{equation}
  \label{eq:xtikigen}
  x_\mu = \pa{ A^T A + L_\mu^T L_\mu }^{-1} A^T \tilde b
\end{equation}

If we now take the SVD of $A$ and use $L_\mu = \mu I$ we get
\[  x_\mu = \pa{ V \Sigma ^2 V^T + \mu ^2 I }^{-1} V \Sigma U^T \tilde b  \]
\[  x_\mu = V^T \pa{\Sigma ^2 + \mu ^2 I }^{-1} \Sigma U^T \tilde b \]
\[  x_\mu = V^T
  \begin{pmatrix}
    \frac{\sigma _1 }{\sigma _1 ^2 + \mu ^2} \\
    & \frac{\sigma _2 }{\sigma _2 ^2 + \mu ^2} \\
    & & \ddots \\
    & & & \frac{\sigma _k }{\sigma _k ^2 + \mu ^2}\\
    & & & & 0\\
    & & & & & \ddots \\
    & & & & & & 0
  \end{pmatrix}
  U^T \tilde  b \]

The filter factors are
\begin{equation}
  \label{eq:tikfilter}
  \varphi ^{\text{Tikhonov}} _{\mu,i} = \frac{\sigma _i ^2}{\sigma _i
    ^2 + \mu ^2}
\end{equation}

Analyzing thw filter factors we see that Tikhonov regularization on
one hand doesn't ignore any component of $\tilde b$, on the other it
dumps all components of it.

\subsection{Tikhonov regularization with the new regularization matrix}

A tradeoff between the two method is shown in \cite{modtikh}. It is
constructed applying Tikhonov regularization using the following
regularization matrix:
\begin{equation*}
  \label{eq:modtikL}
  L_\mu = D _\mu V^T =
  \begin{pmatrix}
    \sqrt{\max\set{\mu ^2 - \sigma _1 ^2,0}} \\
    & \sqrt{\max\set{\mu ^2 - \sigma _2 ^2,0}} \\
    & & \ddots
    & & & \sqrt{\max\set{\mu ^2 - \sigma _n ^2,0}} \\
  \end{pmatrix}
  V^T
\end{equation*}

The solution for this problem is:
\begin{align*}
  x_\mu =& \pa{ A^T A + L_\mu^T L_\mu }^{-1} A^T \tilde b = \\
  = & \pa{ V \Sigma ^2 V^T + V D_\mu ^2 V^T} ^{-1} V\Sigma U^T \tilde
  b = \\
  = & V \pa{ \Sigma ^2 + D_\mu ^2 } ^{-1} \Sigma U^T \tilde b
\end{align*}
\begin{equation}
  \label{eq:modtikx}
  x_\mu  = \sum _{i=1} ^k \frac{\sigma _i}{\sigma _i ^2 + \max\set{
      \mu ^2 - \sigma _i ^2 ,0}  }  v_i \ang{u_i,\tilde b}
\end{equation}

The filter factors are:
\begin{equation}
  \label{eq:modtikfilter}
  \varphi ^{\text{new}} _{\mu,i} = \frac{\sigma _i ^2}{\sigma _i ^2 + \max\set{
      \mu ^2 - \sigma _i ^2 ,0}  } = \left\{
  \begin{matrix}
    1\; & \sigma _i \ge \mu \\
    \frac{\sigma _i ^2}{\mu ^2} \; & \sigma _i < \mu
  \end{matrix}
  \right.
\end{equation}

So this method, like the TSVD, doesn't dump any component with $\sigma
_i \ge \mu$ and, like Tikhonov with $L_\mu = \mu I$, it dumps the
other factors without setting them to $0$.

Since we would like the matrix $L_\mu ^T L_\mu$ to be of small norm
(so that we can get an accurate approximation of $x$), we will show
that $L_\mu ^T L_\mu$ is optimal in Frobenius norm (which is given by
$\norm{M}_F = \sqrt{ \mathrm{trace} \pa{ M^T M } }$ for a generic
matrix $M$)

\begin{myteo}
  Let $M \in \mathbb{R}^{n\times n}$ be a symmetric matrix with
  spectral factorization $M = V \Lambda V^T$, where $V \in
  \mathbb{R}^{n \times n}$ is orthonormal and $\Lambda = \mathrm{diag} \pa{
    \lambda _1, ... \lambda _n}$. Assume that $\mu \ge \min _{1\le
    j\le n} \lambda _j$, let the diagonal matrix $C_\mu = \mathrm{diag}
  \pa{ c_1 , ... c_n}\in \mathbb{R}^{n\times n}$ have the entries $c_j
  = \max \set{\mu - \lambda _j, 0}$.
  
  Then the matrix $M + V C_\mu V^T$ has the smallest eigenvalue $\mu$
  and the distance in the Frobenius norm between $M$ and the closest
  symmetric matrix with the smallest eigenvalue $\mu$ is $\norm{ C_\mu
  }_F$
\end{myteo}

Recalling that the solution, for a generic $L_\mu$ penalization
matrix, is given by
\[ x = \pa{ A^T A + L_\mu^T L_\mu }^{-1} A^T \tilde b \] we are
interested in finding the ``best'' matrix $L_\mu^T L_\mu$ such that
$A^T A + L_\mu^T L_\mu$ has $\mu$ as the smallest eigenvale, the best
choice is the closest matrix to $A^T A$. So if we apply the theorem
with $M = A^T A$ and $C_\mu = D_\mu ^2$ then we see that $A_\mu ^T
A_\mu + L_\mu ^T L_\mu = M + V C_\mu V^T$ is optimal in the Frobenius
norm.

\begin{mycor}
  If $\sigma _1,\mu > 0$ then
  \[ \norm{ L_\mu }^2 _F < \norm{ \mu I} ^2_F \]
\end{mycor}

This corollary shows us that we are doing better than the standard
Tikhonov method.

\section{Choose the regularization parameter: discrepancy principle}
\label{sec:discrepancy}

We pointed out that good results from regularization methods depends
on the choice of the regularization parameter, this can be a number
(integer or real) or a vector. We will consider integer number (for
TSVD) or real number (for Tikhonov regularization).

Various regularization methods has been studied, for example:
\begin{itemize}
\item Discrepancy principle
\item L-curve method
\item General Cross Validation (GCV)
\end{itemize}

We will focus on the first

\subsection{Discrepancy principle}

This method (introduced in \cite{discreporig}) can be applied when a
fairly accurate approximation of the error $\norm{\varepsilon}$ is
available, in this case we take the ``most regular'' solution that
generate an error compatible with $\norm{\varepsilon}$. In other terms
if $\lambda$ is our regularization parameter (and for sake of
simplicity we assume that a bigger $\lambda$ represent a more regular
solution) then we take
\begin{equation}
  \label{eq:discrepancygen}
  \lambda _\varepsilon = \max \set { \lambda :\; \norm{ Ax_\lambda -
      \tilde b} \le \eta \norm{\varepsilon}} 
\end{equation}
where $\eta > 1$ is a user defined constant and $x_\lambda$ is the
solution given by the regularization method with $\lambda$ as
regularization parameter.

In \cite{discrepbook} is shown that if $\norm{\varepsilon} \to 0$ then
$x _\varepsilon \to x$

If we have a regularization method that can be written with filter
factors the discrepancy can be written as
\begin{align*}
  \norm{ Ax_\lambda - \tilde b } =& \norm{ U \Sigma V^T V F \Sigma
    ^{-1} U^T \tilde b - \tilde b} =\\
  =& \norm { U F U^T \tilde b - \tilde b} =\\
  = & \norm{ F U^T \tilde b - U^T \tilde b } =\\
  =& \norm{ \pa{F-I} U^T \tilde b} =\\
  =& \sqrt{\sum _{i=1} ^n \bra{ \pa{ \varphi _i -1} \ang{ u_i, \tilde b} } ^2}
\end{align*}
where $F = \mathrm{diag}\pa{ \varphi _1 ,... , \varphi _n}$ with
$\varphi _i$ are the filters factor for the choosen method (we set
$\varphi _i = 0$ if $\sigma _i = 0$).

If we are using the TSVD the discrepancy for the parameter $\alpha$ is
given by
\[ \sqrt{\sum _{i=\alpha +1} ^n \bra{ \ang{ u_i, \tilde b} } ^2 } \]
and it can be calculated in $O(n^2)$ total time ($O(n)$ if we already
know $U^T \tilde b$).

For Tikhonov regularization we can use the Newton method to find a
zero for the function
\[ f(\mu) = \sum _{i=1} ^n \bra{ \pa{ \varphi _{\mu,i} -1} \ang{ u_i,
    \tilde b} } ^2 - \eta ^2 \norm{\varepsilon} ^2 \] 
the first derivative for this function is
\[ f'(\mu) = 2 \sum _{i=1} ^n \pa{ \varphi _{\mu,i} -1} \varphi
_{\mu,i} ' \ang{ u_i, \tilde b} ^2 \]



\newpage
\bibliographystyle{alpha}
\bibliography{inversesturm,atmospheric,discrepbook,discreporig,modtikh}


\end{document}

